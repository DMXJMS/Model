{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CosyVoice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 11:23:03,199 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\n",
      "2024-08-09 11:23:03,201 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-08-09 11:23:03,268 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 750810b2d0c33f9be9cca0301d47437a and a total number of 980 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer is not installed, please install it if you want to use related modules\n",
      "failed to import ttsfrd, use WeTextProcessing instead\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./third_party/CosyVoice/')\n",
    "\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice\n",
    "from cosyvoice.utils.file_utils import load_wav\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/project_1_plus/lib/python3.8/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "2024-08-09 11:29:38.536599556 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2024-08-09 11:29:38.536611217 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n",
      "2024-08-09 11:29:38,680 WETEXT INFO found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\n",
      "2024-08-09 11:29:38,680 WETEXT INFO found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\n",
      "INFO:wetext-zh_normalizer:found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\n",
      "2024-08-09 11:29:38,681 WETEXT INFO                     /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2024-08-09 11:29:38,681 WETEXT INFO                     /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\n",
      "INFO:wetext-zh_normalizer:                    /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2024-08-09 11:29:38,681 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2024-08-09 11:29:38,681 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "INFO:wetext-zh_normalizer:skip building fst for zh_normalizer ...\n",
      "2024-08-09 11:29:39,048 WETEXT INFO found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_tagger.fst\n",
      "2024-08-09 11:29:39,048 WETEXT INFO found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_tagger.fst\n",
      "INFO:wetext-en_normalizer:found existing fst: /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_tagger.fst\n",
      "2024-08-09 11:29:39,049 WETEXT INFO                     /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\n",
      "2024-08-09 11:29:39,049 WETEXT INFO                     /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\n",
      "INFO:wetext-en_normalizer:                    /opt/conda/envs/project_1_plus/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\n",
      "2024-08-09 11:29:39,050 WETEXT INFO skip building fst for en_normalizer ...\n",
      "2024-08-09 11:29:39,050 WETEXT INFO skip building fst for en_normalizer ...\n",
      "INFO:wetext-en_normalizer:skip building fst for en_normalizer ...\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('./third_party/CosyVoice/third_party/Matcha-TTS/')\n",
    "\n",
    "cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-Instruct')\n",
    "\n",
    "# zero_shot usage\n",
    "prompt_speech_16k = load_wav('asset/zero_shot_lx_prompt.wav', 16000)\n",
    "output = cosyvoice.inference_zero_shot(tts_text='收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', \n",
    "                                       prompt_text='对立开来，但其实你可以想一想，在开车的过程中，是在交通规则的保护下驾驶更自由，还是随心所欲无视规则开车更自由呢？因此我们会发', \n",
    "                                       prompt_speech_16k=prompt_speech_16k)\n",
    "torchaudio.save('zero_shot.wav', output['tts_speech'], 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SenseVoice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "from funasr.utils.postprocess_utils import rich_transcription_postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the latest version of funasr-1.1.4\n",
      "Loading remote code successfully: ./third_party/SenseVoice-main/model.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 15:07:40,047 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\n",
      "2024-08-09 15:07:40,048 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-08-09 15:07:40,135 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 750810b2d0c33f9be9cca0301d47437a and a total number of 980 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer is not installed, please install it if you want to use related modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 15:07:40,361 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-08-09 15:07:40,363 - modelscope - INFO - Use user-specified model revision: master\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'pretrained_models/SenseVoiceSmall'\n",
    "\n",
    "model = AutoModel(\n",
    "    model=model_dir,\n",
    "    trust_remote_code=True,\n",
    "    remote_code=\"./third_party/SenseVoice-main/model.py\",    \n",
    "    vad_model=\"fsmn-vad\",\n",
    "    vad_kwargs={\"max_single_segment_time\": 30000},\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.019: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00,  9.27it/s]                                                                                          \n",
      "rtf_avg: 0.009: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00, 19.41it/s]\n",
      "rtf_avg: 0.010, time_speech:  5.616, time_escape: 0.057: 100%|\u001b[31m██████████\u001b[0m| 1/1 [00:00<00:00, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开饭时间早上9点至下午5点。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# en\n",
    "res = model.generate(\n",
    "    input=f\"{model.model_path}/example/zh.mp3\",\n",
    "    cache={},\n",
    "    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
    "    use_itn=True,\n",
    "    batch_size_s=60,\n",
    "    merge_vad=True,  #\n",
    "    merge_length_s=15,\n",
    ")\n",
    "text = rich_transcription_postprocess(res[0][\"text\"])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./third_party/SenseVoice-main/')\n",
    "from model import SenseVoiceSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading remote code successfully: model\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'pretrained_models/SenseVoiceSmall'\n",
    "\n",
    "m_recogize, param_recogize = SenseVoiceSmall.from_pretrained(model=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    global m_recogize, param_recogize\n",
    "    print('Processing audio:', audio)\n",
    "    res = m_recogize.inference(\n",
    "            data_in=audio,\n",
    "            language=\"zh\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
    "            use_itn=False,**param_recogize)\n",
    "    result = res[0][0]['text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio: /workspace/project_1_plus/asset/zero_shot_lx_prompt.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|zh|><|NEUTRAL|><|Speech|><|woitn|>对立开来但其实你可以想一想在开车的过程中是在交通规则的保护下驾驶更自由还是随心所欲无视规则开车更自由呢因此我们会发'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_audio('/workspace/project_1_plus/asset/zero_shot_lx_prompt.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1207412447.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ollama-python import OllamaClient\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from ollama-python import OllamaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_ollama_service(content):\n",
    "    response = requests.post(\n",
    "        \"http://127.0.0.1:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": \"qwen:0.5b\",\n",
    "            \"prompt\": content,\n",
    "            \"format\": \"json\", \n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    # print(response)\n",
    "    # print(response.status_code, type(response.status_code))\n",
    "    # if response.status_code == 200:\n",
    "    #     return response.json()[\"response\"]\n",
    "    # else:\n",
    "    #     return \"Error: Unable to get response from Ollama service.\"\n",
    "    print(response)\n",
    "    if response.status_code == 200:\n",
    "        response_content = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                response_content += json.loads(line)[\"message\"][\"content\"]\n",
    "        return response_content\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to_ollama(message, port=11434):\n",
    "    url = f\"http://localhost:{port}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": \"qwen:0.5b\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": message}]\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response_content = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                response_content += json.loads(line)[\"message\"][\"content\"]\n",
    "        return response_content\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好！有什么可以帮助您的吗？'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_message_to_ollama('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是阿里云研发的大规模语言模型，我叫通义千问。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_message_to_ollama('你是谁？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我正在帮助用户解答问题。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_message_to_ollama('你刚才回答了什么问题？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_1_plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
